# ğŸ§  Chicago Crimes Data Pipeline â€“ Airflow + Soda + PostgreSQL

## ğŸ“‹ Project Overview
This project implements a **complete data pipeline** using **Apache Airflow** (via Astro CLI), **Soda Core** for data quality validation, and **PostgreSQL** for storage.  
It automates the process of **ingesting**, **validating**, **transforming**, and **loading** public crime data from the **City of Chicago API**.

---

## ğŸš€ Architecture

### ğŸ§° Tools & Technologies

| Tool | Role |
|------|------|
| ğŸŒ€ **Airflow (Astro CLI)** | Orchestrates ETL tasks |
| ğŸ§ª **Soda Core** | Validates data quality before and after transformation |
| ğŸ—„ï¸ **PostgreSQL** | Stores raw and cleaned datasets |
| ğŸŒ **Public API (Chicago Crimes)** | Data source |

---

## ğŸ—ï¸ Pipeline Structure

### DAG: `chicago_crimes_pipeline`

| Step | Task ID | Description |
|------|----------|-------------|
| 1ï¸âƒ£ | `ingest_api` | Fetches data from the **Chicago Crimes API** and stores it in the PostgreSQL raw table |
| 2ï¸âƒ£ | `soda_scan_raw` | Performs Soda data quality checks on **raw data** |
| 3ï¸âƒ£ | `transform_and_load` | Cleans and transforms data before loading into the cleaned table |
| 4ï¸âƒ£ | `soda_scan_clean` | Runs final Soda quality checks on **transformed data** |
| 5ï¸âƒ£ | *(optional)* `load_to_postgres` | Final load of validated data into PostgreSQL for analytics |

---

## ğŸ“‚ Project Structure

app_docker/
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ chicago_crimes_pipeline.py # Main Airflow DAG
â”‚ â”œâ”€â”€ soda/
â”‚ â”‚ â”œâ”€â”€ configuration.yml # Soda configuration
â”‚ â”‚ â””â”€â”€ checks/
â”‚ â”‚ â”œâ”€â”€ raw_chicago_crimes.yml # Soda checks for raw data
â”‚ â”‚ â””â”€â”€ crimes_clean.yml # Soda checks for cleaned data
â”œâ”€â”€ data/ # (optional) Local storage
â”œâ”€â”€ docker-compose.override.yml # Custom Docker setup
â”œâ”€â”€ airflow_settings.yaml # Airflow connection settings
â”œâ”€â”€ Dockerfile # Custom Airflow image build
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Documentation


### 1ï¸âƒ£ Start the environment
astro dev start
Airflow UI â†’ http://localhost:8080

PostgreSQL â†’ localhost:5432

Default credentials â†’ postgres/postgres

### 2ï¸âƒ£ Verify running containers
astro dev ps

### 3ï¸âƒ£ Run the pipeline manually

Go to the Airflow UI and trigger the DAG:

chicago_crimes_pipeline

### 4ï¸âƒ£ View Soda results

Open the logs of soda_scan_raw and soda_scan_clean to view data quality validation summaries.

## âœ… Data Quality Checks (Soda)
raw_chicago_crimes.yml
checks for raw_chicago_crimes:
  - row_count > 0
  - missing_count(id) = 0
  - duplicate_count(id) = 0
  - invalid_percent(primary_type) < 20 %:
      valid values:
        - THEFT
        - ASSAULT
        - BATTERY
        - ROBBERY

crimes_clean.yml
checks for crimes_clean:
  - row_count > 0
  - missing_count(id) = 0
  - duplicate_count(id) = 0
  - freshness(ts) < 48h
  - invalid_percent(latitude) = 0:
      valid min: -90
      valid max: 90
  - invalid_percent(longitude) = 0:
      valid min: -180
      valid max: 180

### ğŸ“Š Results

âœ… Fully automated end-to-end pipeline
âœ… Soda quality checks before and after transformation
âœ… Seamless integration between Airflow, PostgreSQL, and Soda
âœ… Ready for extension to other public datasets